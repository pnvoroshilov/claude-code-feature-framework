# ğŸ§ª Testing Workflow - Hybrid Testing Modes

âš ï¸ **This applies to DEVELOPMENT MODE only. SIMPLE mode has no Testing status.**

## ğŸ“‹ Testing Mode Configuration

**This project supports TWO testing modes controlled by `manual_mode` setting:**

- **ğŸ”µ MANUAL MODE** (`manual_mode = true`) - UC-04 Variant B
  - User performs manual testing
  - Test servers started for user access
  - Testing URLs saved for persistence
  - User manually transitions status after testing

- **ğŸŸ¢ AUTOMATED MODE** (`manual_mode = false`) - UC-04 Variant A
  - Testing agents write and execute tests automatically
  - Tests run in isolated environment
  - Reports generated in `/Tests/Report`
  - Auto-transition based on test results

## ğŸ” Check Testing Mode BEFORE Starting

**FIRST, check project settings to determine which mode to use:**

```bash
mcp__claudetask__get_project_settings
```

Look for: `"Manual Testing Mode": True` or `False`

## When Testing Phase Starts

**Testing status is triggered when**:
- Task transitions from "In Progress" to "Testing"
- Implementation is detected as complete
- User manually updates status to "Testing"

**After detecting Testing phase, follow the workflow for the configured mode below.**

---

# ğŸ”µ MANUAL TESTING MODE (`manual_mode = true`)

## ğŸš¨ğŸš¨ğŸš¨ CRITICAL TESTING URL REQUIREMENT ğŸš¨ğŸš¨ğŸš¨

**â›” FAILURE TO SAVE TESTING URLs = CRITICAL ERROR**
**You MUST save testing URLs IMMEDIATELY after starting test servers**
**This is NOT optional - it is MANDATORY for task tracking**

## ğŸ“‹ MANUAL TESTING CHECKLIST (ALL STEPS REQUIRED)

When task moves to "Testing" status in MANUAL mode:

### Step 1: Find Available Ports
```bash
# Check if default ports are occupied
lsof -i :3333  # Backend default
lsof -i :3000  # Frontend default

# If occupied, find free ports in ranges:
# Backend: 3333-5000
# Frontend: 3000-4000
```

### Step 2: Start Backend Server
```bash
cd worktrees/task-{id}
python -m uvicorn app.main:app --port FREE_BACKEND_PORT --reload &
```

### Step 3: Start Frontend Server
```bash
cd worktrees/task-{id}
PORT=FREE_FRONTEND_PORT npm start &
```

### Step 4: ğŸ”´ SAVE TESTING URLs (MANDATORY - DO NOT SKIP)

**âš ï¸ YOU MUST EXECUTE THIS COMMAND IMMEDIATELY:**

```bash
mcp__claudetask__set_testing_urls --task_id={id} \
  --urls='{"frontend": "http://localhost:FREE_FRONTEND_PORT", "backend": "http://localhost:FREE_BACKEND_PORT"}'
```

**â›” DO NOT PROCEED WITHOUT SAVING URLs**
**â›” THIS IS NOT OPTIONAL - IT IS REQUIRED**
**â›” SKIPPING THIS STEP = TASK TRACKING FAILURE**

### Step 5: Save Stage Result (ONLY AFTER URLs ARE SAVED)

```bash
mcp__claudetask__append_stage_result --task_id={id} --status="Testing" \
  --summary="Testing environment prepared with URLs saved" \
  --details="Backend: http://localhost:FREE_BACKEND_PORT
Frontend: http://localhost:FREE_FRONTEND_PORT
âœ… URLs SAVED to task database for persistent access
Ready for manual testing"
```

### Step 6: Notify User WITH CONFIRMATION

```
âœ… Testing environment ready and URLs SAVED to task:
- Backend: http://localhost:FREE_BACKEND_PORT
- Frontend: http://localhost:FREE_FRONTEND_PORT
- URLs permanently saved to task #{id} for easy access

Please perform manual testing and update status when complete.
```

### Step 7: Wait for User Testing

- âœ… User will manually test the implementation
- âœ… User will update status when testing is complete
- âŒ DO NOT delegate to testing agents
- âŒ DO NOT auto-transition to next status

## âš ï¸ VALIDATION: If You Setup Test Environment WITHOUT Saving URLs

**This is a CRITICAL ERROR that must be fixed:**

The task tracking is INCOMPLETE if URLs are not saved because:
- User cannot access test URLs later
- URLs are not persisted in database
- Task status is incomplete

**If you forgot to save URLs, FIX IT IMMEDIATELY:**
```bash
# Get the actual ports from running processes
lsof -i :PORT_NUMBER

# Save URLs now
mcp__claudetask__set_testing_urls --task_id={id} \
  --urls='{"frontend": "http://localhost:ACTUAL_PORT", "backend": "http://localhost:ACTUAL_PORT"}'
```

## Manual Mode Restrictions

âŒ **DO NOT delegate to testing agents** - This is for MANUAL testing only
âŒ **DO NOT create automated tests** - Unless explicitly requested
âŒ **DO NOT auto-transition** - Wait for user to update status
âŒ **DO NOT run test commands** - User will test manually

## Manual Mode Status Exit

**User will update status when testing is complete:**

- If tests pass â†’ User updates to "Code Review"
- If bugs found â†’ User updates to "In Progress" to fix
- If major issues â†’ User may update to "Analysis" to re-evaluate

**You should NEVER auto-transition from Testing status in MANUAL mode.**

---

# ğŸŸ¢ AUTOMATED TESTING MODE (`manual_testing_mode = false`)

## ğŸ“‹ AUTOMATED TESTING CHECKLIST

When task moves to "Testing" status in AUTOMATED mode:

### Step 1: Read Analysis Documents
```bash
# Get task context
mcp__claudetask__get_task --task_id={id}

# Read analysis docs in worktree
cat worktrees/task-{id}/Analyze/Requirements/*
cat worktrees/task-{id}/Analyze/Design/*
```

### Step 2: Determine Test Types
Based on analysis docs and DoD, determine which tests are needed:
- âœ… UI/Frontend tests (web-tester agent)
- âœ… Backend/API tests (quality-engineer agent)
- âœ… Integration tests (if multiple components changed)

### Step 3: Delegate to Testing Agents

**For Frontend/UI Testing:**
```bash
# Use web-tester agent for E2E browser testing
mcp__claudetask__delegate_to_agent \
  --task_id={id} \
  --agent_type="web-tester" \
  --instructions="Read /Analyze docs and DoD. Create and execute UI tests per test plan. Save results in /Tests/Report/ui-tests.md"
```

**For Backend Testing:**
```bash
# Use quality-engineer for backend/API testing
mcp__claudetask__delegate_to_agent \
  --task_id={id} \
  --agent_type="quality-engineer" \
  --instructions="Read /Analyze docs and DoD. Create pytest tests for backend APIs. Test all endpoints from test plan. Run tests and save results in /Tests/Report/backend-tests.md"
```

### Step 4: Wait for Test Results

Monitor agent completion and collect test reports from:
- `/Tests/Report/ui-tests.md`
- `/Tests/Report/backend-tests.md`

### Step 5: Analyze Test Results

Review all test reports and determine:
- âœ… All tests passed â†’ Proceed to Step 6
- âŒ Critical failures â†’ Return to "In Progress"
- âš ï¸ Minor issues â†’ Document and proceed (or return based on severity)

### Step 6: Save Stage Result

```bash
mcp__claudetask__append_stage_result --task_id={id} --status="Testing" \
  --summary="Automated testing completed" \
  --details="UI Tests: [PASS/FAIL count]
Backend Tests: [PASS/FAIL count]
Total: [X passed, Y failed]
Reports: /Tests/Report/*.md"
```

### Step 7: Auto-Transition Status

**Based on test results:**

```bash
# If all tests passed
mcp__claudetask__update_status --task_id={id} --status="Code Review" \
  --comment="All automated tests passed"

# If critical issues found
mcp__claudetask__update_status --task_id={id} --status="In Progress" \
  --comment="Critical test failures: [list issues]"
```

## Automated Mode Workflow

âœ… **DO delegate to testing agents** - Use web-tester, python-expert
âœ… **DO create automated tests** - Required for automated mode
âœ… **DO auto-transition** - Based on test results
âœ… **DO generate test reports** - Save in `/Tests/Report/`

## Automated Mode Status Exit

**Auto-transition based on test results:**

- All tests pass â†’ Auto-update to "Code Review"
- Critical failures â†’ Auto-update to "In Progress" with details
- Blocking issues â†’ May return to "Analysis" if design flaws found

**You SHOULD auto-transition from Testing status in AUTOMATED mode.**

## Port Management Best Practices

### Default Ports:
- Backend: 3333
- Frontend: 3000

### If Ports Occupied:
1. Check with `lsof -i :PORT`
2. Find alternative ports in safe ranges
3. Use ports that won't conflict with other services

### Port Ranges to Use:
- Backend: 3333-5000 (avoid system ports < 3000)
- Frontend: 3000-4000 (avoid 8000+, often used by other tools)

## Troubleshooting

### Port Already in Use:
```bash
# Find process using port
lsof -i :PORT_NUMBER

# Kill old process if safe
kill PID_NUMBER

# Or use different port
```

### Server Won't Start:
- Check worktree directory exists
- Ensure dependencies installed
- Check for syntax errors in code
- Look at server logs for errors

### Frontend Not Accessible:
- Check PORT environment variable
- Ensure npm start completed successfully
- Check browser console for errors
- Verify REACT_APP_BACKEND_URL if needed

## Complete Example

```bash
# 1. Check ports
lsof -i :3333
lsof -i :3000

# 2. Start backend (port 3333 free)
cd worktrees/task-42
python -m uvicorn app.main:app --port 3333 --reload &

# 3. Start frontend (port 3000 occupied, use 3001)
PORT=3001 npm start &

# 4. ğŸ”´ MANDATORY: Save URLs
mcp__claudetask__set_testing_urls --task_id=42 \
  --urls='{"frontend": "http://localhost:3001", "backend": "http://localhost:3333"}'

# 5. Save stage result
mcp__claudetask__append_stage_result --task_id=42 --status="Testing" \
  --summary="Testing environment ready with URLs saved" \
  --details="Backend: http://localhost:3333
Frontend: http://localhost:3001
âœ… URLs saved to database"

# 6. Notify user
echo "âœ… Testing environment ready:
- Backend: http://localhost:3333
- Frontend: http://localhost:3001
- URLs saved to task #42"
```

---

# ğŸ“Š Mode Comparison Summary

| Feature | Manual Mode (true) | Automated Mode (false) |
|---------|-------------------|------------------------|
| **Who Tests** | User manually | Testing agents |
| **Test Servers** | âœ… Started for user access | âŒ Not needed |
| **Testing URLs** | ğŸ”´ MUST save URLs | âŒ Not required |
| **Test Reports** | User documents findings | Auto-generated in `/Tests/Report/` |
| **Status Transition** | User manually updates | Auto-transition based on results |
| **Delegation** | âŒ Forbidden | âœ… Required |
| **Test Creation** | âŒ Not created | âœ… Agents write tests |

## Decision Tree

```
Task enters "Testing" status
    â†“
Check: mcp__claudetask__get_project_settings
    â†“
manual_testing_mode = ?
    â†“
    â”œâ”€â†’ TRUE (Manual Mode)
    â”‚   â”œâ”€â†’ Find free ports
    â”‚   â”œâ”€â†’ Start test servers
    â”‚   â”œâ”€â†’ ğŸ”´ SAVE testing URLs (mandatory!)
    â”‚   â”œâ”€â†’ Save stage result
    â”‚   â”œâ”€â†’ Notify user
    â”‚   â””â”€â†’ WAIT for user to update status
    â”‚
    â””â”€â†’ FALSE (Automated Mode)
        â”œâ”€â†’ Read analysis docs
        â”œâ”€â†’ Determine test types
        â”œâ”€â†’ Delegate to testing agents
        â”œâ”€â†’ Wait for test reports
        â”œâ”€â†’ Analyze results
        â”œâ”€â†’ Save stage result
        â””â”€â†’ AUTO-TRANSITION based on results
```

## Key Reminders by Mode

### Manual Mode Checklist:
1. âœ… Find free ports
2. âœ… Start servers
3. âœ… **SAVE URLs** (mandatory!)
4. âœ… Save stage result
5. âœ… Notify user
6. âœ… Wait for user testing
7. âŒ NEVER auto-transition

**The `set_testing_urls` command is NOT optional in Manual Mode - it MUST be called for proper task tracking.**

### Automated Mode Checklist:
1. âœ… Read analysis documents
2. âœ… Determine test types
3. âœ… Delegate to testing agents
4. âœ… Wait for test completion
5. âœ… Analyze test results
6. âœ… Save stage result with test summary
7. âœ… Auto-transition based on results
8. âœ… Create test reports in `/Tests/Report/`

**Testing agents MUST be used in Automated Mode - manual testing is not applicable.**
