# Best Practices

Industry-standard practices for database migrations that prevent data loss, minimize downtime, and maintain code quality across development teams.

## Table of Contents

- [Migration Naming Conventions](#migration-naming-conventions)
- [When to Use Autogenerate vs Manual](#when-to-use-autogenerate-vs-manual)
- [Reviewing Generated Migrations](#reviewing-generated-migrations)
- [Making Migrations Reversible](#making-migrations-reversible)
- [Atomic Migrations](#atomic-migrations)
- [Index Creation Without Locking](#index-creation-without-locking)
- [Testing Migrations](#testing-migrations)
- [Documenting Complex Changes](#documenting-complex-changes)
- [Team Collaboration](#team-collaboration)
- [Data Integrity](#data-integrity)
- [Performance Considerations](#performance-considerations)
- [Security Best Practices](#security-best-practices)

## Migration Naming Conventions

### Descriptive Names

**Good**: Clear, action-oriented names
```bash
alembic revision --autogenerate -m "add_email_column_to_users"
alembic revision --autogenerate -m "create_orders_table"
alembic revision --autogenerate -m "add_index_on_user_email"
alembic revision --autogenerate -m "migrate_user_names_to_full_name"
```

**Bad**: Vague or generic names
```bash
alembic revision -m "update"
alembic revision -m "changes"
alembic revision -m "fix"
alembic revision -m "temp"
```

### Naming Format

Use consistent format across your team:

**Snake case** (recommended):
```
add_email_to_users
create_products_table
remove_legacy_columns
```

**Kebab case**:
```
add-email-to-users
create-products-table
```

**Sentence case**:
```
"Add email to users"
"Create products table"
```

### Action Verbs

Start with clear action verbs:

| Verb | Use When |
|------|----------|
| `add` | Adding columns, tables, constraints |
| `create` | Creating new tables or indexes |
| `remove`/`drop` | Deleting columns, tables, constraints |
| `rename` | Renaming columns or tables |
| `modify`/`alter` | Changing column types or properties |
| `migrate` | Transforming data |
| `populate` | Adding initial/seed data |
| `update` | Updating existing data |

### Examples with Context

```bash
# Table operations
create_users_table
drop_legacy_sessions_table
rename_customers_to_clients

# Column operations
add_created_at_to_orders
remove_unused_columns_from_products
modify_email_column_length

# Index operations
add_index_on_order_status
create_composite_index_on_user_lookup
drop_unused_indexes

# Data migrations
migrate_user_roles_to_permissions
populate_default_settings
update_product_categories

# Constraints
add_foreign_key_orders_to_users
add_unique_constraint_on_email
add_check_constraint_on_age
```

## When to Use Autogenerate vs Manual

### Use Autogenerate For

‚úÖ **Standard table/column operations**:
```python
# Model change detected by autogenerate
class User(Base):
    __tablename__ = 'users'
    email = Column(String(255))  # New column

# Command:
alembic revision --autogenerate -m "add_email_to_users"
```

‚úÖ **Simple type changes**:
```python
# Change from String(100) to String(255)
username = Column(String(255))  # Was String(100)
```

‚úÖ **Adding/removing indexes**:
```python
email = Column(String(255), index=True)  # Auto-detects index
```

‚úÖ **Foreign key relationships**:
```python
user_id = Column(Integer, ForeignKey('users.id'))
```

### Write Manual Migrations For

‚ùå **Column/table renames**:
```python
# Autogenerate sees this as drop + create
# Write manual migration instead
def upgrade():
    op.alter_column('users', 'name', new_column_name='full_name')

def downgrade():
    op.alter_column('users', 'full_name', new_column_name='name')
```

‚ùå **Data transformations**:
```python
def upgrade():
    # Combine first_name and last_name
    op.execute("""
        UPDATE users
        SET full_name = first_name || ' ' || last_name
    """)
```

‚ùå **Complex type changes**:
```python
def upgrade():
    # String to Integer with data validation
    op.execute("""
        UPDATE users SET age_temp = CAST(age AS INTEGER)
        WHERE age ~ '^[0-9]+$'
    """)
    op.drop_column('users', 'age')
    op.alter_column('users', 'age_temp', new_column_name='age')
```

‚ùå **Database-specific features**:
```python
def upgrade():
    # PostgreSQL-specific: Create enum type
    op.execute("CREATE TYPE user_status AS ENUM ('active', 'inactive', 'banned')")
    op.add_column('users', sa.Column('status', sa.Enum('active', 'inactive', 'banned', name='user_status')))
```

‚ùå **Zero-downtime changes**:
```python
def upgrade():
    # Phase 1: Add nullable column
    op.add_column('users', sa.Column('email', sa.String(255), nullable=True))
    # Phase 2 will populate data
    # Phase 3 will add NOT NULL constraint
```

## Reviewing Generated Migrations

### Always Review Autogenerated Migrations

**Never blindly trust autogenerate**. Always review the generated file:

```bash
# Generate migration
alembic revision --autogenerate -m "add_email"

# IMPORTANT: Review this file before applying!
# alembic/versions/xxx_add_email.py
```

### Review Checklist

‚úÖ **Verify operations are correct**:
```python
def upgrade():
    # Check: Is this the intended change?
    op.add_column('users', sa.Column('email', sa.String(255), nullable=True))

    # Check: Should there be an index?
    # Autogenerate may miss index=True in model
    op.create_index('ix_users_email', 'users', ['email'])
```

‚úÖ **Check for missing operations**:
```python
# Model has index=True but autogenerate missed it
class User(Base):
    email = Column(String(255), index=True)

# Add manually to migration:
def upgrade():
    op.add_column('users', sa.Column('email', sa.String(255)))
    op.create_index('ix_users_email', 'users', ['email'])  # Add this
```

‚úÖ **Verify nullable/default handling**:
```python
# Generated (may fail on existing data):
op.add_column('users', sa.Column('email', sa.String(255), nullable=False))

# Fixed (add default or make nullable first):
op.add_column('users', sa.Column('email', sa.String(255), nullable=True))
# Or:
op.add_column('users', sa.Column('email', sa.String(255), server_default=''))
```

‚úÖ **Check foreign key constraints**:
```python
# Ensure parent table/column exists
op.create_foreign_key(
    'fk_orders_user_id',
    'orders', 'users',
    ['user_id'], ['id'],
    ondelete='CASCADE'  # Add delete behavior
)
```

‚úÖ **Validate downgrade() is correct**:
```python
def upgrade():
    op.add_column('users', sa.Column('email', sa.String(255)))
    op.create_index('ix_users_email', 'users', ['email'])

def downgrade():
    # Reverse order: drop index first, then column
    op.drop_index('ix_users_email')
    op.drop_column('users', 'email')
```

### Common Autogenerate Issues

**Issue 1: Doesn't detect renames**
```python
# Autogenerate produces:
def upgrade():
    op.drop_column('users', 'name')
    op.add_column('users', sa.Column('full_name', sa.String(200)))

# Fix: Manual rename keeps data
def upgrade():
    op.alter_column('users', 'name', new_column_name='full_name')
```

**Issue 2: Misses partial indexes**
```python
# Model:
email = Column(String(255), index=True)
# But you want partial index

# Autogenerate creates:
op.create_index('ix_users_email', 'users', ['email'])

# Fix: Add PostgreSQL partial index
op.create_index(
    'ix_users_active_email',
    'users',
    ['email'],
    postgresql_where=sa.text("status = 'active'")
)
```

**Issue 3: Enum handling varies by database**
```python
# PostgreSQL requires explicit enum type creation
def upgrade():
    # Add this manually:
    op.execute("CREATE TYPE status_enum AS ENUM ('active', 'inactive')")
    op.add_column('users', sa.Column('status', sa.Enum('active', 'inactive', name='status_enum')))
```

## Making Migrations Reversible

### Why Reversibility Matters

**Scenario**: Deploy migration, discover critical bug, need to rollback.

**Without reversible migration**:
```python
def downgrade():
    pass  # or raise NotImplementedError
# Now you can't rollback!
# Must manually fix database or restore from backup
```

**With reversible migration**:
```python
def downgrade():
    # Proper reversal of upgrade()
    op.drop_column('users', 'email')

# Can safely rollback:
$ alembic downgrade -1
```

### Writing Reversible Migrations

**Simple reversibility**:
```python
def upgrade():
    op.add_column('users', sa.Column('email', sa.String(255)))

def downgrade():
    op.drop_column('users', 'email')
```

**Complex reversibility** (preserve data):
```python
def upgrade():
    # Rename column
    op.alter_column('users', 'name', new_column_name='full_name')

def downgrade():
    # Reverse the rename
    op.alter_column('users', 'full_name', new_column_name='name')
```

**Partial reversibility** (acknowledge data loss):
```python
def upgrade():
    op.drop_column('users', 'temporary_token')

def downgrade():
    # Can recreate column structure but not data
    op.add_column('users', sa.Column('temporary_token', sa.String(100)))
    # Note: Original token values are lost
```

### When Irreversibility Is Acceptable

Sometimes downgrade() can't fully restore state:

```python
def upgrade():
    # Combine first_name + last_name into full_name
    op.add_column('users', sa.Column('full_name', sa.String(200)))
    op.execute("""
        UPDATE users
        SET full_name = first_name || ' ' || last_name
    """)
    op.drop_column('users', 'first_name')
    op.drop_column('users', 'last_name')

def downgrade():
    # Can split full_name but loses middle names, suffixes, etc.
    op.add_column('users', sa.Column('first_name', sa.String(100)))
    op.add_column('users', sa.Column('last_name', sa.String(100)))
    op.execute("""
        UPDATE users
        SET first_name = split_part(full_name, ' ', 1),
            last_name = split_part(full_name, ' ', 2)
    """)
    op.drop_column('users', 'full_name')
    # Document: This loses data fidelity
```

**Document irreversibility**:
```python
def downgrade():
    """
    PARTIAL REVERSIBILITY WARNING:
    This downgrade splits full_name back to first_name/last_name,
    but loses middle names, prefixes, suffixes, and non-Western name formats.
    Complex names will be truncated.

    Only use this downgrade if absolutely necessary.
    Consider restoring from backup instead.
    """
    # ... downgrade operations
```

## Atomic Migrations

### Transaction Wrapping

Alembic automatically wraps migrations in transactions (for most databases):

```python
# This is atomic - all-or-nothing
def upgrade():
    op.create_table('users', ...)
    op.create_table('orders', ...)
    op.create_foreign_key(...)
    # If foreign key fails, tables are rolled back
```

### Benefits of Atomicity

- **Consistency**: Database never in partial state
- **Safety**: Failed migrations don't leave debris
- **Simplicity**: No manual cleanup needed

### Non-Atomic Operations

Some operations cannot run in transactions:

**PostgreSQL: CREATE INDEX CONCURRENTLY**
```python
def upgrade():
    # Must run outside transaction
    op.execute('CREATE INDEX CONCURRENTLY ix_users_email ON users(email)')
```

**Solution**: Disable transaction for this migration
```python
# At top of migration file
from alembic import op
import sqlalchemy as sa

# Revision identifiers
revision = 'abc123'
down_revision = 'xyz789'
branch_labels = None
depends_on = None

def upgrade():
    # Manually commit any pending transaction
    connection = op.get_bind()
    connection.execute('COMMIT')

    # Now run non-transactional command
    connection.execute('CREATE INDEX CONCURRENTLY ix_users_email ON users(email)')

def downgrade():
    connection = op.get_bind()
    connection.execute('COMMIT')
    connection.execute('DROP INDEX CONCURRENTLY ix_users_email')
```

### Keeping Migrations Small

**Best practice**: One logical change per migration

**Good**:
```python
# Migration 1: Add column
def upgrade():
    op.add_column('users', sa.Column('email', sa.String(255)))

# Migration 2: Add index
def upgrade():
    op.create_index('ix_users_email', 'users', ['email'])
```

**Bad** (hard to reason about, hard to rollback):
```python
def upgrade():
    # Unrelated changes in one migration
    op.add_column('users', sa.Column('email', sa.String(255)))
    op.create_table('products', ...)
    op.alter_column('orders', 'status', ...)
    op.execute("UPDATE settings SET value = 'new'")
```

Small migrations are:
- Easier to review
- Safer to deploy
- Simpler to rollback
- Faster to execute
- Easier to debug

## Index Creation Without Locking

### The Locking Problem

Standard index creation locks the table:

```sql
-- Blocks all writes to users table until complete
CREATE INDEX ix_users_email ON users(email);
-- On large table: could take minutes or hours
-- Application writes fail during this time
```

### PostgreSQL Solution: CONCURRENTLY

```python
def upgrade():
    # Create index without blocking writes
    op.create_index(
        'ix_users_email',
        'users',
        ['email'],
        postgresql_concurrently=True
    )

def downgrade():
    op.drop_index(
        'ix_users_email',
        postgresql_concurrently=True
    )
```

**Important**: Must disable transaction (see above).

### Complete Example

```python
"""Add index on user email without locking

Revision ID: abc123
Revises: xyz789
Create Date: 2025-01-31 10:30:00
"""

from alembic import op
import sqlalchemy as sa

revision = 'abc123'
down_revision = 'xyz789'

def upgrade():
    # Exit transaction for CONCURRENTLY
    connection = op.get_bind()
    connection.execute('COMMIT')

    # Create index without blocking
    connection.execute(
        'CREATE INDEX CONCURRENTLY ix_users_email ON users(email)'
    )

def downgrade():
    connection = op.get_bind()
    connection.execute('COMMIT')
    connection.execute('DROP INDEX CONCURRENTLY ix_users_email')
```

### MySQL/MariaDB Alternative

MySQL 5.6+ supports online index creation:

```python
def upgrade():
    op.create_index(
        'ix_users_email',
        'users',
        ['email'],
        mysql_algorithm='INPLACE',  # Online operation
        mysql_lock='NONE'           # Don't lock table
    )
```

## Testing Migrations

### Local Testing Workflow

```bash
# 1. Start with clean database matching production schema
alembic upgrade head

# 2. Create new migration
alembic revision --autogenerate -m "add_email"

# 3. Review migration file
# Edit alembic/versions/xxx_add_email.py

# 4. Test upgrade
alembic upgrade head
# Check database schema: SELECT * FROM users;
# Run application to verify compatibility

# 5. Test downgrade
alembic downgrade -1
# Verify schema reverted correctly
# Run application to verify it still works

# 6. Re-apply to continue development
alembic upgrade head
```

### Automated Migration Tests

```python
# tests/test_migrations.py
import pytest
from alembic import command
from alembic.config import Config
from sqlalchemy import create_engine, inspect

def test_upgrade_downgrade():
    """Test migration can upgrade and downgrade"""
    config = Config("alembic.ini")

    # Start from one revision back
    command.downgrade(config, "-1")
    current = get_current_revision()

    # Upgrade
    command.upgrade(config, "head")
    assert get_current_revision() != current

    # Downgrade
    command.downgrade(config, "-1")
    assert get_current_revision() == current

def test_schema_matches_models():
    """Verify database schema matches SQLAlchemy models"""
    from myapp.models import Base

    engine = create_engine("postgresql://...")
    inspector = inspect(engine)

    # Check all model tables exist
    model_tables = set(Base.metadata.tables.keys())
    db_tables = set(inspector.get_table_names())
    assert model_tables == db_tables

    # Check columns match for each table
    for table_name in model_tables:
        model_columns = set(Base.metadata.tables[table_name].columns.keys())
        db_columns = set(c['name'] for c in inspector.get_columns(table_name))
        assert model_columns == db_columns, f"Column mismatch in {table_name}"
```

### Testing with Real Data

```python
def test_data_migration():
    """Test data transformation preserves critical data"""
    from sqlalchemy import create_engine, text

    engine = create_engine("postgresql://test-db")

    # Insert test data before migration
    with engine.begin() as conn:
        conn.execute(text("""
            INSERT INTO users (first_name, last_name)
            VALUES ('John', 'Doe'), ('Jane', 'Smith')
        """))

    # Run migration
    config = Config("alembic.ini")
    command.upgrade(config, "head")

    # Verify data transformation
    with engine.begin() as conn:
        result = conn.execute(text("SELECT full_name FROM users"))
        names = [row[0] for row in result]
        assert 'John Doe' in names
        assert 'Jane Smith' in names
```

### Staging Environment Testing

**Before production**, always test in staging:

1. **Restore production data to staging**
   ```bash
   pg_dump production_db | psql staging_db
   ```

2. **Run migration on staging**
   ```bash
   alembic upgrade head
   ```

3. **Verify application works**
   ```bash
   # Run full test suite
   pytest

   # Manual QA testing
   # Load test if needed
   ```

4. **Time the migration**
   ```bash
   time alembic upgrade head
   # Plan production maintenance window accordingly
   ```

## Documenting Complex Changes

### When to Add Extra Documentation

Add detailed comments for:
- Data transformations
- Multi-step migrations
- Irreversible changes
- Performance considerations
- Database-specific operations

### Documentation Example

```python
"""Migrate user names from first_name/last_name to full_name

This migration consolidates the separate first_name and last_name columns
into a single full_name column to support international name formats.

IMPORTANT NOTES:
- This migration is partially irreversible. Downgrade will attempt to split
  full_name back to first/last, but this loses data fidelity for:
  * Middle names and initials
  * Name prefixes (Dr., Mr., etc.)
  * Name suffixes (Jr., III, etc.)
  * Non-Western name orders (surname-first cultures)

- Data transformation happens in batch mode (1000 rows at a time) to avoid
  locking the entire table. Migration may take 5-10 minutes on production.

- Application must be updated to use full_name field before this migration runs.
  Deploy application v2.3.0+ first, then run migration.

ROLLBACK PLAN:
If issues occur, downgrade is available but lossy. Better to restore from
backup taken before migration.

Revision ID: abc123def456
Revises: prev_revision
Create Date: 2025-01-31 10:30:15.123456
"""

from alembic import op
import sqlalchemy as sa

revision = 'abc123def456'
down_revision = 'prev_revision'

def upgrade():
    """
    Consolidate first_name and last_name into full_name.

    Steps:
    1. Add full_name column (nullable)
    2. Populate full_name from existing names
    3. Wait for application deployment (manual step)
    4. Drop old columns (will be in next migration)
    """

    # Step 1: Add new column
    op.add_column(
        'users',
        sa.Column('full_name', sa.String(200), nullable=True)
    )

    # Step 2: Populate full_name
    # Use batch processing for large tables
    connection = op.get_bind()
    connection.execute(sa.text("""
        UPDATE users
        SET full_name = TRIM(CONCAT(first_name, ' ', last_name))
        WHERE full_name IS NULL
    """))

    # Note: Do NOT drop old columns yet!
    # Next migration (after app deployment) will drop first_name/last_name

def downgrade():
    """
    Split full_name back to first_name and last_name.

    WARNING: This is lossy and only handles simple "First Last" format.
    Middle names, prefixes, suffixes, and non-Western names will be truncated.
    """

    # Add old columns back
    op.add_column(
        'users',
        sa.Column('first_name', sa.String(100), nullable=True)
    )
    op.add_column(
        'users',
        sa.Column('last_name', sa.String(100), nullable=True)
    )

    # Split full_name (lossy operation)
    connection = op.get_bind()
    connection.execute(sa.text("""
        UPDATE users
        SET first_name = SPLIT_PART(full_name, ' ', 1),
            last_name = SUBSTRING(full_name FROM POSITION(' ' IN full_name) + 1)
        WHERE first_name IS NULL
    """))

    # Drop full_name column
    op.drop_column('users', 'full_name')
```

## Team Collaboration

### Pull Request Process

**PR Checklist for Migrations**:

‚úÖ Migration file included in PR
‚úÖ Models updated to match migration
‚úÖ Migration tested locally (upgrade + downgrade)
‚úÖ Migration description is clear
‚úÖ Complex changes documented in migration docstring
‚úÖ Reviewers tagged who understand database implications
‚úÖ Deployment plan documented if non-standard

**Example PR Description**:
```markdown
## Add email column to users table

### Changes
- Added `email` column to User model (nullable, unique, indexed)
- Generated Alembic migration

### Testing
- [x] Tested upgrade locally
- [x] Tested downgrade locally
- [x] Tested with existing data
- [x] Verified application still works

### Deployment Notes
- This migration is safe to run on production without downtime
- Estimated execution time: <1 second
- Column is nullable to support existing users
- Application will prompt existing users for email after deployment

### Rollback Plan
- Simple downgrade available: `alembic downgrade -1`
- No data loss on rollback

### Reviewer Notes
Please verify:
- [ ] Migration only adds column (no data transformation)
- [ ] Index creation is included
- [ ] Downgrade properly reverses changes
```

### Handling Conflicts

**Scenario**: Two developers create migrations simultaneously

```bash
# Developer A creates migration with parent xyz789
revision = 'abc111'
down_revision = 'xyz789'

# Developer B creates migration with same parent xyz789
revision = 'abc222'
down_revision = 'xyz789'

# Now we have a branched history!
```

**Resolution**:
```bash
# 1. Identify the conflict
$ alembic branches
# Shows branched revisions

# 2. Create merge migration
$ alembic merge -m "merge migrations" abc111 abc222
# Creates new migration with both as parents

# 3. Review merge migration
# Edit generated file

# 4. Test thoroughly
$ alembic upgrade head
```

### Communication Guidelines

**Notify team before**:
- Large migrations (>1 minute execution)
- Schema changes affecting multiple services
- Irreversible data transformations
- Changes requiring application updates

**Example notification**:
```
üóÉÔ∏è Database Migration Notice

Migration: Add full_name to users (consolidate first/last names)
Deployment: 2025-02-01 at 02:00 UTC
Estimated time: 10-15 minutes
Downtime: None (zero-downtime deployment)

Required actions:
1. Deploy application v2.3.0 before migration
2. Run migration: alembic upgrade head
3. Verify: Check /health endpoint shows users with full_name

Rollback plan: alembic downgrade -1 (lossy, backup available)

Questions? #database-migrations Slack channel
```

## Data Integrity

### Validating Data Before Constraints

**Problem**: Adding NOT NULL constraint to column with null values fails.

**Solution**: Validate and fix data first.

```python
def upgrade():
    # Step 1: Add column as nullable
    op.add_column(
        'users',
        sa.Column('email', sa.String(255), nullable=True)
    )

    # Step 2: Set default for existing rows
    connection = op.get_bind()
    connection.execute(sa.text("""
        UPDATE users
        SET email = username || '@example.com'
        WHERE email IS NULL
    """))

    # Step 3: Now safe to add NOT NULL constraint
    op.alter_column('users', 'email', nullable=False)
```

### Maintaining Referential Integrity

**Always add foreign keys with proper constraints**:

```python
def upgrade():
    # Add user_id column
    op.add_column(
        'orders',
        sa.Column('user_id', sa.Integer(), nullable=True)
    )

    # Populate user_id from existing data
    # (Assuming you have a way to match orders to users)

    # Add foreign key constraint
    op.create_foreign_key(
        'fk_orders_user_id',
        'orders', 'users',
        ['user_id'], ['id'],
        ondelete='CASCADE'  # Delete orders when user deleted
    )

    # Make NOT NULL after data populated
    op.alter_column('orders', 'user_id', nullable=False)
```

### Check Constraints for Data Quality

```python
def upgrade():
    # Ensure age is reasonable
    op.create_check_constraint(
        'ck_users_age_valid',
        'users',
        'age >= 0 AND age <= 150'
    )

    # Ensure email format
    op.create_check_constraint(
        'ck_users_email_format',
        'users',
        "email ~ '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'"
    )

    # Ensure proper status values
    op.create_check_constraint(
        'ck_orders_status_valid',
        'orders',
        "status IN ('pending', 'processing', 'shipped', 'delivered', 'cancelled')"
    )
```

## Performance Considerations

### Minimize Lock Time

**Bad** (locks table for entire operation):
```python
def upgrade():
    # Adds column and populates - table locked entire time
    op.add_column('users', sa.Column('score', sa.Integer(), nullable=False, server_default='0'))
    connection = op.get_bind()
    connection.execute(sa.text("UPDATE users SET score = calculate_score(id)"))
```

**Good** (minimize lock time):
```python
def upgrade():
    # Add column with default (fast, minimal lock)
    op.add_column('users', sa.Column('score', sa.Integer(), nullable=True))

    # Populate in batches (outside main migration if possible)
    connection = op.get_bind()
    batch_size = 1000
    offset = 0

    while True:
        result = connection.execute(sa.text(f"""
            UPDATE users
            SET score = calculate_score(id)
            WHERE id IN (
                SELECT id FROM users WHERE score IS NULL
                ORDER BY id LIMIT {batch_size}
            )
        """))
        if result.rowcount == 0:
            break

    # Add NOT NULL after population
    op.alter_column('users', 'score', nullable=False)
```

### Batch Processing

For large data migrations:

```python
def upgrade():
    connection = op.get_bind()
    batch_size = 5000
    offset = 0

    while True:
        # Process in chunks
        result = connection.execute(sa.text(f"""
            UPDATE users
            SET full_name = first_name || ' ' || last_name
            WHERE id > {offset} AND id <= {offset + batch_size}
        """))

        if result.rowcount == 0:
            break

        offset += batch_size

        # Optional: Add progress logging
        print(f"Processed {offset} rows...")
```

### Index Strategy

**Create indexes after bulk inserts**:
```python
def upgrade():
    # Create table without indexes
    op.create_table(
        'analytics_events',
        sa.Column('id', sa.Integer(), primary_key=True),
        sa.Column('user_id', sa.Integer()),
        sa.Column('event_type', sa.String(50)),
        sa.Column('created_at', sa.DateTime())
    )

    # Bulk load data (if needed)
    # load_analytics_data()

    # Create indexes after data loaded
    op.create_index('ix_events_user_id', 'analytics_events', ['user_id'])
    op.create_index('ix_events_created_at', 'analytics_events', ['created_at'])
```

## Security Best Practices

### Handling Sensitive Data

**Never log sensitive data in migrations**:

```python
# Bad
def upgrade():
    connection = op.get_bind()
    users = connection.execute(sa.text("SELECT id, password_hash FROM users"))
    print(f"Migrating {len(users)} users with hashes...")  # Logs sensitive data!

# Good
def upgrade():
    connection = op.get_bind()
    result = connection.execute(sa.text("SELECT COUNT(*) FROM users"))
    count = result.scalar()
    print(f"Migrating {count} users...")  # No sensitive data
```

### Database Permissions

Run migrations with least privilege:

```sql
-- Create migration user with limited permissions
CREATE USER alembic_user WITH PASSWORD 'secure_password';

-- Grant only necessary permissions
GRANT CONNECT ON DATABASE mydb TO alembic_user;
GRANT USAGE ON SCHEMA public TO alembic_user;
GRANT CREATE ON SCHEMA public TO alembic_user;
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO alembic_user;

-- For production: read-only for most tables
GRANT SELECT ON sensitive_data TO alembic_user;
```

### Audit Trail

Log migration execution:

```python
def upgrade():
    connection = op.get_bind()

    # Log migration start
    connection.execute(sa.text("""
        INSERT INTO migration_audit (
            revision, action, executed_at, executed_by
        ) VALUES (
            :revision, 'upgrade', NOW(), CURRENT_USER
        )
    """), {'revision': revision})

    # Perform migration operations
    op.add_column(...)

    # Log migration complete
    connection.execute(sa.text("""
        UPDATE migration_audit
        SET completed_at = NOW(), status = 'success'
        WHERE revision = :revision AND action = 'upgrade'
    """), {'revision': revision})
```

## Next Steps

- See [patterns.md](patterns.md) for common migration patterns
- Review [examples/](../examples/) for practical implementations
- Check [workflows.md](../resources/workflows.md) for deployment processes
